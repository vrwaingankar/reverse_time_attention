{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gztr6NddGUVc",
        "outputId": "ab6096a7-e8ff-407a-ac68-013895cd1734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAN-KLAqGQ0-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as L\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.constraints import non_neg, Constraint\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    precision_recall_curve,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define global variables for configuration parameters with default values\n",
        "NUM_CODES = 1000\n",
        "NUMERIC_SIZE = 0\n",
        "USE_TIME = False\n",
        "EMB_SIZE = 200\n",
        "EPOCHS = 1\n",
        "N_STEPS = 300\n",
        "RECURRENT_SIZE = 200\n",
        "PATH_DATA_TRAIN = \"/content/drive/MyDrive/MIMIC/output/data_train.pkl\"  # Replace with your file path\n",
        "PATH_DATA_TEST = \"/content/drive/MyDrive/MIMIC/output/data_test.pkl\"  # Replace with your file path\n",
        "PATH_TARGET_TRAIN = \"/content/drive/MyDrive/MIMIC/output/target_train.pkl\"  # Replace with your file path\n",
        "PATH_TARGET_TEST = \"/content/drive/MyDrive/MIMIC/output/target_test.pkl\"  # Replace with your file path\n",
        "BATCH_SIZE = 32\n",
        "DROPOUT_INPUT = 0.0\n",
        "DROPOUT_CONTEXT = 0.0\n",
        "L2 = 0.0\n",
        "DIRECTORY = \"/content/drive/MyDrive/MIMIC/output/Model\"  # Replace with your file path\n",
        "ALLOW_NEGATIVE = False"
      ],
      "metadata": {
        "id": "1Jy-JySgKD-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceBuilder(Sequence):\n",
        "    \"\"\"\n",
        "    Class to properly construct data into sequences prior to training.\n",
        "\n",
        "    :param Sequence: Customized Sequence class for generating batches of data\n",
        "    :type Sequence: :class:`tensorflow.keras.utils.Sequence`\n",
        "    :returns: Padded, dense data used for Sequence construction (codes,visits,numerics)\n",
        "    :rtype: :class:`ndarray`\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, target, batch_size, target_out=True):\n",
        "        \"\"\"\n",
        "        Instantiates the code.\n",
        "\n",
        "        :param data: Training data sequences (codes, visits, numerics)\n",
        "        :type data: list[:class:`ndarray`]\n",
        "        :param target: List of target values\n",
        "        :type target: :class:`numpy.ndarray`\n",
        "        :param batch_size: Number of samples in each batch\n",
        "        :type batch_size: int\n",
        "        :param target_out: If `True` (default), then return the target values\n",
        "        :type target_out: bool\n",
        "        :returns: data sequences (codes, visits, numerics)\n",
        "        :rtype: list[:class:`ndarray`]\n",
        "        \"\"\"\n",
        "\n",
        "        # Receive all appropriate data\n",
        "        self.codes = data[0]\n",
        "        index = 1\n",
        "        if NUMERIC_SIZE:\n",
        "            self.numeric = data[index]\n",
        "            index += 1\n",
        "\n",
        "        if USE_TIME:\n",
        "            self.time = data[index]\n",
        "\n",
        "        self.num_codes = NUM_CODES\n",
        "        self.target = target\n",
        "        self.batch_size = batch_size\n",
        "        self.target_out = target_out\n",
        "        self.numeric_size = NUMERIC_SIZE\n",
        "        self.use_time = USE_TIME\n",
        "        self.n_steps = N_STEPS\n",
        "        # self.balance = (1-(float(sum(target))/len(target)))/(float(sum(target))/len(target))\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Compute number of batches.\n",
        "        Add extra batch if the data doesn't exactly divide into batches\n",
        "\n",
        "        :return: Number of batches per epoch\n",
        "        :rtype: int\n",
        "        \"\"\"\n",
        "\n",
        "        if len(self.codes) % self.batch_size == 0:\n",
        "            return len(self.codes) // self.batch_size\n",
        "        return len(self.codes) // self.batch_size + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get batch of specific index.\n",
        "\n",
        "        :param idx: The index number for the batch to return\n",
        "        :type idx: int\n",
        "        :return: Padded data sequences (codes, visits, numerics)\n",
        "        :rtype: list[:class:`ndarray`]\n",
        "        \"\"\"\n",
        "\n",
        "        def pad_data(data, length_visits, length_codes, pad_value=0):\n",
        "            \"\"\"\n",
        "            Pad numpy array to shift sparse matrix to dense matrix\n",
        "\n",
        "            :param data: Training data sequences (codes, visits, numerics)\n",
        "            :type data: list[:class:`ndarray`]\n",
        "            :param int length_visits: max visit count in batch\n",
        "            :param int length_codes: max codes length in batch\n",
        "            :param pad_value: numeric value to represent padding, defaults to 0\n",
        "            :type pad_value: int, optional\n",
        "            :return: 'dense' array with padding for codes and visits\n",
        "            :rtype: :class:`numpy.ndarray`\n",
        "            \"\"\"\n",
        "\n",
        "            zeros = np.full((len(data), length_visits, length_codes), pad_value)\n",
        "            for steps, mat in zip(data, zeros):\n",
        "                if steps != [[-1]]:\n",
        "                    for step, mhot in zip(steps, mat[-len(steps) :]):\n",
        "                        # Populate the data into the appropriate visit\n",
        "                        mhot[: len(step)] = step\n",
        "\n",
        "            return zeros\n",
        "\n",
        "        # Compute reusable batch slice\n",
        "        batch_slice = slice(idx * self.batch_size, (idx + 1) * self.batch_size)\n",
        "        x_codes = self.codes[batch_slice]\n",
        "        # Max number of visits and codes inside the visit for this batch\n",
        "        pad_length_visits = min(max(map(len, x_codes)), self.n_steps)\n",
        "        pad_length_codes = max(map(lambda x: max(map(len, x)), x_codes))\n",
        "        # Number of elements in a batch (useful in case of partial batches)\n",
        "        length_batch = len(x_codes)\n",
        "        # Pad data\n",
        "        x_codes = pad_data(x_codes, pad_length_visits, pad_length_codes, self.num_codes)\n",
        "        outputs = [x_codes]\n",
        "        # Add numeric data if necessary\n",
        "        if self.numeric_size:\n",
        "            x_numeric = self.numeric[batch_slice]\n",
        "            x_numeric = pad_data(x_numeric, pad_length_visits, self.numeric_size, -99.0)\n",
        "            outputs.append(x_numeric)\n",
        "        # Add time data if necessary\n",
        "        if self.use_time:\n",
        "            x_time = sequence.pad_sequences(\n",
        "                self.time[batch_slice],\n",
        "                dtype=np.float32,\n",
        "                maxlen=pad_length_visits,\n",
        "                value=+99,\n",
        "            ).reshape(length_batch, pad_length_visits, 1)\n",
        "            outputs.append(x_time)\n",
        "\n",
        "        # Add target if necessary (training vs validation)\n",
        "        if self.target_out:\n",
        "            target = self.target[batch_slice].reshape(length_batch, 1, 1)\n",
        "            # sample_weights = (target*(self.balance-1)+1).reshape(length_batch, 1)\n",
        "            # In our experiments sample weights provided worse results\n",
        "            return (outputs, target)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "Wu_qWSEaGp-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FreezePadding_Non_Negative(Constraint):\n",
        "    \"\"\"\n",
        "    Freezes the last weight to be near 0 - permit negative weights.\n",
        "\n",
        "    :param Constraint: Keras sequence constraint\n",
        "    :type Constraint: :class:`tensorflow.keras.constraints.Constraint`\n",
        "    :return: padded tensor or variable\n",
        "    :rtype: :class:`tensorflow.Tensor`\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, w):\n",
        "        other_weights = K.cast(K.greater_equal(w, 0)[:-1], K.floatx())\n",
        "        last_weight = K.cast(\n",
        "            K.equal(K.reshape(w[-1, :], (1, K.shape(w)[1])), 0.0), K.floatx()\n",
        "        )\n",
        "        appended = K.concatenate([other_weights, last_weight], axis=0)\n",
        "        w *= appended\n",
        "        return w"
      ],
      "metadata": {
        "id": "vC4J_JLqHNa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FreezePadding(Constraint):\n",
        "    \"\"\"\n",
        "    Freezes the last weight to be near 0 - don't permit negative weights.\n",
        "\n",
        "    :param Constraint: Keras sequence constraint\n",
        "    :type Constraint: :class:`tensorflow.keras.constraints.Constraint`\n",
        "    :return: padded tensor or variable\n",
        "    :rtype: :class:`tensorflow.Tensor`\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, w):\n",
        "        other_weights = K.cast(K.ones(K.shape(w))[:-1], K.floatx())\n",
        "        last_weight = K.cast(\n",
        "            K.equal(K.reshape(w[-1, :], (1, K.shape(w)[1])), 0.0), K.floatx()\n",
        "        )\n",
        "        appended = K.concatenate([other_weights, last_weight], axis=0)\n",
        "        w *= appended\n",
        "        return w"
      ],
      "metadata": {
        "id": "TeSGq-7RHeRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data():\n",
        "    \"\"\"Read the data from provided paths and assign it into lists\"\"\"\n",
        "\n",
        "    data_train_df = pd.read_pickle(PATH_DATA_TRAIN)\n",
        "    data_test_df = pd.read_pickle(PATH_DATA_TEST)\n",
        "    y_train = pd.read_pickle(PATH_TARGET_TRAIN)[\"target\"].values\n",
        "    y_test = pd.read_pickle(PATH_TARGET_TEST)[\"target\"].values\n",
        "    data_output_train = [data_train_df[\"codes\"].values]\n",
        "    data_output_test = [data_test_df[\"codes\"].values]\n",
        "\n",
        "    if NUMERIC_SIZE:\n",
        "        data_output_train.append(data_train_df[\"numerics\"].values)\n",
        "\n",
        "    if USE_TIME:\n",
        "        data_output_train.append(data_train_df[\"to_event\"].values)\n",
        "\n",
        "    return (data_output_train, y_train, data_output_test, y_test)"
      ],
      "metadata": {
        "id": "ymMAYKsdHiJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_create():\n",
        "    \"\"\"\n",
        "    Create tensorflow DAG for training a model, and then compile/train\n",
        "    the model at the end.\n",
        "\n",
        "    :return: trained/compiled Keras model\n",
        "    :rtype: :class:`tensorflow.keras..Model`\n",
        "    \"\"\"\n",
        "\n",
        "    def retain():\n",
        "        \"\"\"\n",
        "        Helper function to create DAG of Keras Layers via functional API approach.\n",
        "        The Keras Layer design is mimicking RETAIN architecture.\n",
        "        :return: Keras model\n",
        "        :rtype: :class:`tensorflow.keras.Model`\n",
        "        \"\"\"\n",
        "\n",
        "        # Define the constant for model saving\n",
        "        reshape_size = EMB_SIZE + NUMERIC_SIZE\n",
        "        if ALLOW_NEGATIVE:\n",
        "            embeddings_constraint = FreezePadding()\n",
        "            beta_activation = \"tanh\"\n",
        "            output_constraint = None\n",
        "        else:\n",
        "            embeddings_constraint = FreezePadding_Non_Negative()\n",
        "            beta_activation = \"sigmoid\"\n",
        "            output_constraint = non_neg()\n",
        "\n",
        "        def reshape(data):\n",
        "            \"\"\"Reshape the context vectors to 3D vector\"\"\"\n",
        "            return K.reshape(x=data, shape=(K.shape(data)[0], 1, reshape_size))\n",
        "\n",
        "        # Code Input\n",
        "        codes = L.Input((None, None), name=\"codes_input\")\n",
        "        inputs_list = [codes]\n",
        "        # Calculate embedding for each code and sum them to a visit level\n",
        "        codes_embs_total = L.Embedding(\n",
        "            NUM_CODES + 1, EMB_SIZE, name=\"embedding\"\n",
        "        )(codes)\n",
        "        codes_embs = L.Lambda(lambda x: K.sum(x, axis=2))(codes_embs_total)\n",
        "        # Numeric input if needed\n",
        "        if NUMERIC_SIZE:\n",
        "            numerics = L.Input((None, NUMERIC_SIZE), name=\"numeric_input\")\n",
        "            inputs_list.append(numerics)\n",
        "            full_embs = L.concatenate([codes_embs, numerics], name=\"catInp\")\n",
        "        else:\n",
        "            full_embs = codes_embs\n",
        "\n",
        "        # Apply dropout on inputs\n",
        "        full_embs = L.Dropout(DROPOUT_INPUT)(full_embs)\n",
        "\n",
        "        # Time input if needed\n",
        "        if USE_TIME:\n",
        "            time = L.Input((None, 1), name=\"time_input\")\n",
        "            inputs_list.append(time)\n",
        "            time_embs = L.concatenate([full_embs, time], name=\"catInp2\")\n",
        "        else:\n",
        "            time_embs = full_embs\n",
        "\n",
        "\n",
        "        alpha = L.Bidirectional(\n",
        "            L.LSTM(RECURRENT_SIZE, return_sequences=True, implementation=2),\n",
        "            name=\"alpha\",\n",
        "        )\n",
        "        beta = L.Bidirectional(\n",
        "            L.LSTM(RECURRENT_SIZE, return_sequences=True, implementation=2),\n",
        "            name=\"beta\",\n",
        "        )\n",
        "\n",
        "        alpha_dense = L.Dense(1, kernel_regularizer=l2(L2))\n",
        "        beta_dense = L.Dense(\n",
        "            EMB_SIZE + NUMERIC_SIZE,\n",
        "            activation=beta_activation,\n",
        "            kernel_regularizer=l2(L2),\n",
        "        )\n",
        "\n",
        "        # Compute alpha, visit attention\n",
        "        alpha_out = alpha(time_embs)\n",
        "        alpha_out = L.TimeDistributed(alpha_dense, name=\"alpha_dense_0\")(alpha_out)\n",
        "        alpha_out = L.Softmax(name=\"softmax_1\", axis=1)(alpha_out)\n",
        "        # Compute beta, codes attention\n",
        "        beta_out = beta(time_embs)\n",
        "        beta_out = L.TimeDistributed(beta_dense, name=\"beta_dense_0\")(beta_out)\n",
        "        # Compute context vector based on attentions and embeddings\n",
        "        c_t = L.Multiply()([alpha_out, beta_out, full_embs])\n",
        "        c_t = L.Lambda(lambda x: K.sum(x, axis=1))(c_t)\n",
        "        # Reshape to 3d vector for consistency between Many to Many and Many to One implementations\n",
        "        contexts = L.Lambda(reshape)(c_t)\n",
        "\n",
        "        # Make a prediction\n",
        "        contexts = L.Dropout(DROPOUT_CONTEXT)(contexts)\n",
        "        output_layer = L.Dense(\n",
        "            1,\n",
        "            activation=\"sigmoid\",\n",
        "            name=\"dOut\",\n",
        "            kernel_regularizer=l2(L2),\n",
        "            kernel_constraint=output_constraint,\n",
        "        )\n",
        "\n",
        "        # TimeDistributed is used for consistency\n",
        "        # between Many to Many and Many to One implementations\n",
        "        output = L.TimeDistributed(output_layer, name=\"time_distributed_out\")(contexts)\n",
        "        # Define the model with appropriate inputs\n",
        "        model = Model(inputs=inputs_list, outputs=[output])\n",
        "\n",
        "        return model\n",
        "\n",
        "    # Set Tensorflow to grow GPU memory consumption instead of grabbing all of it at once\n",
        "    K.clear_session()\n",
        "    config = tf.compat.v1.ConfigProto(\n",
        "        allow_soft_placement=True, log_device_placement=False\n",
        "    )\n",
        "    config.gpu_options.allow_growth = True\n",
        "    tfsess = tf.compat.v1.Session(config=config)\n",
        "    tf.compat.v1.keras.backend.set_session(tfsess)\n",
        "    model_final = retain()\n",
        "\n",
        "    # Compile the model - adamax has produced best results in our experiments\n",
        "    model_final.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "        sample_weight_mode=\"temporal\",\n",
        "    )\n",
        "\n",
        "    return model_final"
      ],
      "metadata": {
        "id": "S4174cVwIN1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_callbacks(model, data):\n",
        "    \"\"\"At the end of each epoch, determine various callback statistics (e.g. ROC-AUC)\n",
        "\n",
        "    :param model: Keras model\n",
        "    :type model: :class:`tensorflow.keras.Model`\n",
        "    :param data: Validation data - data sequences (codes, visits, numeric values) and classifier.\n",
        "    :type data: tuple( list( :class:`ndarray`), :class:`ndarray`)\n",
        "    :param ARGS: Arguments object containing user-specified parameters\n",
        "    :type ARGS: :class:`argparse.Namespace`\n",
        "    :return: various callback objects - naming convention for saved HDF5 files, custom logging class, \\\n",
        "    reduced learning rate\n",
        "    :rtype: tuple(:class:`tensorflow.keras.callbacks.ModelCheckpoint`, :class:`LogEval`, \\\n",
        "    :class:`tensorflow.keras.callbacks.ReduceLROnPlateau`)\n",
        "    \"\"\"\n",
        "\n",
        "    class LogEval(Callback):\n",
        "        \"\"\"Logging Callback\"\"\"\n",
        "\n",
        "        def __init__(self, filepath, model, data, interval=1):\n",
        "            \"\"\"Constructor for logging class\n",
        "\n",
        "            :param str filepath: path for log file & Keras HDF5 files\n",
        "            :param model: model from training used for end-of-epoch analytics\n",
        "            :type model: :class:`keras.engine.training.Model`\n",
        "            :param data: Validation data used for end-of-epoch analytics \\\n",
        "            (e.g. data sequences (codes, visits, numerics) and classifier)\n",
        "            :type data: tuple(list[:class:`ndarray`],:class:`ndarray`)\n",
        "            :param ARGS: Arguments object containing user-specified parameters\n",
        "            :type ARGS: :class:`argparse.Namespace`\n",
        "            :param interval: Interval for logging (e.g. every epoch), defaults to 1\n",
        "            :type interval: int, optional\n",
        "            \"\"\"\n",
        "\n",
        "            super(Callback, self).__init__()\n",
        "            self.filepath = filepath\n",
        "            self.interval = interval\n",
        "            self.data_test, self.y_test = data\n",
        "            self.generator = SequenceBuilder(\n",
        "                data=self.data_test,\n",
        "                target=self.y_test,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                target_out=False,\n",
        "            )\n",
        "            self.model = model\n",
        "\n",
        "        def on_epoch_end(self, epoch, logs={}):\n",
        "\n",
        "            # Compute ROC-AUC and average precision the validation data every interval epochs\n",
        "            if epoch % self.interval == 0:\n",
        "\n",
        "                # Generate predictions\n",
        "                preds = []\n",
        "                for x in self.generator:\n",
        "                    batch_pred = self.model.predict_on_batch(\n",
        "                        x=x,\n",
        "                    )\n",
        "                    preds.append(batch_pred.flatten())\n",
        "                y_pred = np.concatenate(preds, axis=0)\n",
        "\n",
        "                # Compute performance\n",
        "                score_roc = roc_auc_score(self.y_test, y_pred)\n",
        "                score_pr = average_precision_score(self.y_test, y_pred)\n",
        "\n",
        "                # Create log file if it doesn't exist, otherwise write to it\n",
        "                if os.path.exists(self.filepath):\n",
        "                    append_write = \"a\"\n",
        "                else:\n",
        "                    append_write = \"w\"\n",
        "                with open(self.filepath, append_write) as file_output:\n",
        "                    file_output.write(\n",
        "                        \"\\nEpoch: {:d}- ROC-AUC: {:.6f} ; PR-AUC: {:.6f}\".format(\n",
        "                            epoch, score_roc, score_pr\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                # Print performance\n",
        "                print(\n",
        "                    \"\\nEpoch: {:d} - ROC-AUC: {:.6f} PR-AUC: {:.6f}\".format(\n",
        "                        epoch, score_roc, score_pr\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    # Create callbacks\n",
        "    if not os.path.exists(DIRECTORY):\n",
        "        os.makedirs(DIRECTORY)\n",
        "    checkpoint = ModelCheckpoint(filepath=DIRECTORY + \"/weights.{epoch:02d}.hdf5\")\n",
        "    log = LogEval(DIRECTORY + \"/log.txt\", model, data)\n",
        "    return (checkpoint, log)"
      ],
      "metadata": {
        "id": "-9TI_v0kIUEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data_train, y_train, data_test, y_test):\n",
        "    \"\"\"\n",
        "    Class to hold callback artifacts, Sequence builder of training data, model training\n",
        "    generator\n",
        "\n",
        "    :param model: Keras model\n",
        "    :type model: :class:`tensorflow.keras.Model`\n",
        "    :param data_train: List with sub-arrays for medical codes, visits, and demographics\n",
        "    :type data_train: list(:class:`numpy.ndarray`)\n",
        "    :param y_train: Array with classifiers for training set\n",
        "    :type y_train: :class:`numpy.ndarray`\n",
        "    :param data_test: List with sub-arrays for medical codes, visits, and demographics\n",
        "    :type data_test: list(:class:`numpy.ndarray`)\n",
        "    :param y_test: Array with classifiers for test set\n",
        "    :type y_test: :class:`numpy.ndarray`\n",
        "    \"\"\"\n",
        "\n",
        "    checkpoint, log = create_callbacks(model, (data_test, y_test))\n",
        "    train_generator = SequenceBuilder(\n",
        "        data=data_train, target=y_train, batch_size=BATCH_SIZE\n",
        "    )\n",
        "    model.fit(\n",
        "        x=train_generator,\n",
        "        epochs=EPOCHS,\n",
        "        max_queue_size=15,\n",
        "        use_multiprocessing=True,\n",
        "        callbacks=[checkpoint, log],\n",
        "        verbose=1,\n",
        "        workers=3,\n",
        "        initial_epoch=0,\n",
        "    )"
      ],
      "metadata": {
        "id": "JgDvBLGrIsZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function\"\"\"\n",
        "    print(\"Reading Data...\")\n",
        "    data_train, y_train, data_test, y_test = read_data()\n",
        "\n",
        "    print(\"Creating Model...\")\n",
        "    model = model_create()\n",
        "\n",
        "    print(\"Training Model...\")\n",
        "    train_model(\n",
        "        model=model,\n",
        "        data_train=data_train,\n",
        "        y_train=y_train,\n",
        "        data_test=data_test,\n",
        "        y_test=y_test,\n",
        "    )"
      ],
      "metadata": {
        "id": "ivzKphfqIusr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGfyvhGIVkL9",
        "outputId": "08d00cf4-7917-4d8a-a47c-3d7731c2b2c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading Data...\n",
            "Creating Model...\n",
            "Training Model...\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.5753 - accuracy: 0.6992"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0 - ROC-AUC: 0.797669 PR-AUC: 0.703878\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r189/189 [==============================] - 19s 66ms/step - loss: 0.5752 - accuracy: 0.6993\n"
          ]
        }
      ]
    }
  ]
}